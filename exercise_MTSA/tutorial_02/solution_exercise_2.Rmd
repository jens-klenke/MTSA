---
title: 'Multivariate Time Series Analysis'
author: 'Dr. Yannick Hoga'
author2: 'Thilo Reinschl√ºssel'
subtitle: 'Solution Exercise Sheet 2'
semester: "Winter Term 2019/2020"
output:
  pdf_document:
    keep_tex: yes
    template: ../template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
linkcolor: black
urlcolor: black
citecolor: black
colorlinks: true
font: Times New Roman
fontsize: 12pt
geometry: lmargin = 2cm, rmargin = 2.5cm, tmargin = 2cm, bmargin = 2.5cm
classoption: a4paper
---

```{r , include=FALSE}
Sys.setlocale(locale = "English_United States.1252") ## English US Windows
knitr::opts_chunk$set(echo = TRUE)

source(here::here("packages/packages.R"))
```

# Exercise 1: Moments and Simulation of a VAR(1) Process

Take the model from Example 2.4 on Slide 2-6:

\begin{itemize}
  \item[a)] Derive a formula to obtain the population cross-covariance matrices for the lags $1$ to $10$ and compute them using R.
  \item[] \textit{Hint: A glance at the slides might save you some time.}
\end{itemize}

_Solution:_ 

\begin{align*}
  z_t & = \phi_1 z_{t-1} + a_t\\
  \\
  \Gamma_0 & = E \left( z_t - \mu  \right) \left( z_t - \mu  \right)^{'}\\
  & = E \left[ \phi_1 \left( z_{t -1} - \mu \right) \left( z_{t -1} - \mu \right)^{'} \phi_1^{'} \right] + E \left[a_t a_t^{'} \right]\\
    & =\phi_1  \ \Gamma_0 \ \phi_1^{'} + \Sigma_a\\
  \Leftrightarrow \vec(\Gamma_0) &= (\phi_1 \otimes \phi_1) \cdot \vec(\Gamma_0) + \vec(\Sigma_a) \\
  \Leftrightarrow \vec(\Gamma_0) &= (I_{K^2} - \phi_1 \otimes \phi_1) \cdot \vec(\Sigma_a) \\
 \Rrightarrow \Gamma_1 & = \phi_1 \Gamma_0\\
 \Rrightarrow \Gamma_l & = \phi_{l-1} \Gamma_{l-1} = \phi_1^{l} \Gamma_0\\
\end{align*}

\begin{itemize}
  \item[b)] Based on your results, compute the cross-correlation matrices.
  \item[] \textit{Hint: A loop might save you some time.}
\end{itemize}

_Solution:_ 

```{r}
# First define parameters/coefficients:
Phi <- matrix(data = c(0.8, -0.3, 0.4, 0.6), 
              byrow = FALSE, nrow = 2) # VAR coefficients

Sigma_a <- matrix(data = c(2, 0.5, 0.5, 1.0), 
                  byrow = FALSE, nrow = 2) # innovations' covariances

Phi_kron <- kronecker(X = Phi, Y = Phi) # kronecker product
 # alternative command: Phi %x% Phi

Ident <- diag(ncol(Phi_kron)) 
# identity matrix with the same dimensions as Phi_kron

Gamma0.vec <- solve(Ident - Phi_kron) %*% c(Sigma_a) 
# c() works like the "vec" operator

Gamma0.mat <- matrix(data = Gamma0.vec, nrow = 2, byrow = FALSE)

mat_names <- c('z_1', 'z_2') # setting names of the series
colnames(Gamma0.mat) <- mat_names # passing column names
rownames(Gamma0.mat) <- mat_names # assig row names
```

The $\Gamma_0$ matrix is then:  

```{r, echo = FALSE}
Gamma0.mat
```

To derive the $\Gamma_1$ matrix, we can simply apply the following formula. 

\begin{align*}
  \Gamma_1 & = \phi_1 \Gamma_0\\
\end{align*}

```{r}
Gamma1.mat <- Phi %*% Gamma0.mat
```

$\Gamma_1$ is then: 

```{r, echo = FALSE}
colnames(Gamma1.mat) <- mat_names
rownames(Gamma1.mat) <- mat_names
Gamma1.mat
```

To derive all $l^{th}$ lagged covariance matrices we can use the more general equation and program a loop over all desired lags.

\begin{align*}
  \Rrightarrow \Gamma_l & = \phi_{l-1} \Gamma_{l-1} = \phi_1^{l} \Gamma_0\\
\end{align*}

To derive the correlation matrices $\rho_l$ we divide the covariances by the standard deviations.

\begin{align*}
 \rho_l & = \left( \rho_{ij} (l)\right)_{i,j = 1}^{K} = D^{-1} \Gamma_l D^{-1}  \\
 \\
   & = 
  \begin{pmatrix}
    \dfrac{\Cov(x_t , x_{t-l})}{\sqrt{\Var(x_t) \cdot \Var(x_{t- l})}} &
    \dfrac{\Cov(x_t , y_{t-l})}{\sqrt{\Var(x_t) \cdot \Var(y_{t- l})}}\\
    \dfrac{\Cov(y_t , x_{t-l})}{\sqrt{\Var(y_t) \cdot \Var(x_{t- l})}} &
    \dfrac{\Cov(y_t , y_{t-l})}{\sqrt{\Var(y_t) \cdot \Var(y_{t- l})}}
  \end{pmatrix}\\
\end{align*}

\begin{itemize}
  \item where $D  = \diag \left\{ \sqrt{\Gamma_{11} (0)}, \ldots , \sqrt{\Gamma_{kk} (0)} \right\}$ 
\end{itemize}



```{r}
# compute further (lagged) cross-covariance-functions
# preparing the variables to store the matrices into 
#covariance
ccovf.list <- list() 
#correlation
ccorf.list <- list() 

# obtaining standard deviations for the single variables (parts of z) 
# and putting them in a diagonal matrix
D.inv <- solve(sqrt(Gamma0.mat * diag(ncol(Phi)))) 

for (i in 1:10){
    
    # covariance
    ccovf <- Phi%^%i %*% Gamma0.mat
    rownames(ccovf) <- mat_names
  ccovf.list[[i]] <- ccovf
    
    # correlation
    ccorf <-  D.inv %*% ccovf.list[[i]] %*% D.inv
    rownames(ccorf) <- mat_names
  ccorf.list[[i]] <- ccorf
}
ccovf.list # cross lagged covariances
ccorf.list # cross lagged correlations
```

\begin{itemize}
  \item[c)] Draw a corresponding innovation sequence $a_t$ for 300 periods from a (multivariate) Gaussian distribution and simulate the given VAR(1) process without any further built-in functions.
  \item[] \textit{Hint: 'mvrnorm' and 'for' are still allowed}
\end{itemize}

_Solution:_ \

\begin{itemize}
  \item Steps:
  \begin{enumerate}
    \item Set $T$ (in code $N$)
    \item Draw $\left\{ a_1, \ldots , a_T \right\} \sim \left[ \mu , \Sigma_a \right]$
    \item Set $z_0 = \mathbb{E} \left( z_t\right)$
    \item $z_1 = \phi_1 z_0 + a_1$
    \item Repeat Step 4 $(T - 1)$ times
    \item (Discard the first few observations to minimize effects of $z_0$ on the results)
    \begin{itemize}
      \item[$\Rightarrow$] \textit{Note} that it is generally advised to discard the first few observations to eliminate the influence of the arbitrary starting point! In our case we skipped this step to keep things short and clear.
    \end{itemize}
  \end{enumerate}
\end{itemize}

```{r simulation study}
# i) Set sample size and further coefficients
N <- 300
# ii) The Basis: Innovations
set.seed(2^9-1) # for replication: the 'random' numbers drawn 
# up from this point will always be the same (given you use 
# the same command to draw them...)
a <- mvrnorm(n = N, mu = c(0,0), Sigma = Sigma_a)
# iii) Writing a function to generate 'z' realisations from 'a' using Phi
var1gen <- function(coef.mat, z.lag, innovation){
  z.current <- coef.mat %*% z.lag + innovation 
  # z_(t) = phi * z_(t-1) + a_(t)
  return(z.current)
}
# iv) Prepare variable for 'z'
z <- ts(data = matrix(data = NA, nrow = (N), ncol = ncol(Phi))
        ,start = 1, names = mat_names)
# v) Set starting values for z (at mean + innovation)
z[1,] <- c(0,0) + a[1,]
# vi) Generate z repeatedly and store it
for (i in 2:(N)){
  z[i,] <- var1gen(coef.mat = Phi, z.lag = z[(i-1),], innovation = a[i,])
}
```

\begin{itemize}
  \item[d)] Plot the multivariate time series you have just created. Does it look stationary?
\end{itemize}

_Solution:_ \

The ordinary organic `plot` command works because $z$ is already a *ts*-class object (among other classes) and not a data frame.

```{r}
plot(z)
```

At least it looks stable hence we cannot rule out stationarity. 


\begin{itemize}
  \item[e)] Estimate the sample cross-covariance and cross-correlation matrices. Compare these with the population moment matrices from task a)
\end{itemize}

_Solution:_ 

\begin{align*}
  \widehat{\Gamma_0} & = \tilde{z}_T^{'} \tilde{z}_{T-1} \cdot (T - 1)^{-1} \\
  \tilde{z}_T^{'} & = z_T - \widehat{\mu}_z\\
  Z & \ \text{is a} \ T \times 2 \ \text{matrix}\\
  z_t &\  \text{is a} \ 2 \times 1 \ \text{vector}\\
  z_t & := 
  \begin{pmatrix}
    x_t \\
    y_t
  \end{pmatrix} \; \leftarrow \text{variables}\\
  Z_t & := 
  \begin{pmatrix}
    X_{t} & Y_{t} \\
    X_{t-1} & Y_{t-1} \\
    X_{t-2} & Y_{t-2} \\
    \vdots & \vdots \\
    X_{t-t} & Y_{t-t} \\
  \end{pmatrix} \; \leftarrow \text{sample data}\\
  \widehat{\Cov(x_t, y_{t-1})} & = \dfrac{1}{(T - 1)} \sum_{t = 1}^{T}  \left( \tilde{X}_t \cdot \tilde{Y}_{t- 1} \right)\\
  \Rightarrow & \ \text{is part of:} \dfrac{1}{T -1} \cdot \tilde{Z}_t^{'} \tilde{Z}_{t-1} = \widehat{\Gamma_1}
\end{align*}


To estimate moments from simulated data one can simply use the command `cov()` respectively `cor()` , but we do it by 'hand' to get a thorough understanding and see the relation to the Yule-Walker equation. 

First we need to demean the multivariate time series. Therefore, we calculate the column means. 

```{r}
mu <- colMeans(z)
```

To demean the series we now have two coding options to proceed with, one would be to iterate over the rows and subtracte the mean vector every time. 

```{r}
z.demeaned <- t(apply(X = z, MARGIN = c(1), FUN = function(x) x - mu))
```

The other option would be to subtracte the column means from each column individually.

```{r}
z.demeaned <- cbind(z[,1]-mu[1], z[,2]-mu[2])
```

Now we are able to compute $\widehat{\Gamma_0}$. 

```{r}
Gamma0.hat <- t(z.demeaned) %*% z.demeaned / (N-1) 
```

Since we already demeaned the series we need to correct the degrees of freedom $(N - 1)$. Please also note that now the first entry is transposed! ( _z.demeaned_ is a data matrix and not a random vector anymore!)

To compute $\widehat{\rho}_0$ we just need to standardize $\widehat{\Gamma}_0$. 

```{r}
standardisation <- matrix(data = c(Gamma0.hat[1,1], 
                                   sqrt( Gamma0.hat[1,1] * Gamma0.hat[2,2]),
                                   sqrt(Gamma0.hat[2,2] * Gamma0.hat[1,1]),
                                   Gamma0.hat[2,2]),
                       nrow = 2, byrow = FALSE) 
Rho0.hat <- Gamma0.hat / standardisation
```

This is just another way to program it, maybe it makes it more visible what is inside $D.inv \ \%*\% \ D.inv$ (from the lecture slides).

Then $\widehat{\rho_0}$ is: 

```{r, echo = FALSE}
colnames(Rho0.hat) <- c('z_1', 'z_2')
rownames(Rho0.hat) <- c('z_1', 'z_2')
Rho0.hat
```

Since we simulated the trajectory with the 'true' values we can compare the estimations with these values. The estimated mean values for the two series are slightly negative $(`r mu - c(0,0)` )^{'}$ so the differences are also slightly negative since we simulated the time series without a mean $\mu_{1,2} = 0$.  

For the covariance $\Gamma_0$ we calculated the analytical solution in part _b_ of this exercise. The differences $(\widehat{\Gamma}_0 - \Gamma_0)$ are: 

```{r, echo = FALSE}
gamma_0.dif <- Gamma0.hat - Gamma0.mat
colnames(gamma_0.dif) <- c('z_1', 'z_2')
rownames(gamma_0.dif) <- c('z_1', 'z_2')
gamma_0.dif
```

The same applies for the comparison of the correlations $\rho_0$ and the estimated correlations $\widehat{\rho_0}$. The differences $(\widehat{\rho}_0 - \rho_0 )$ are: 

```{r, echo = FALSE}
Rho0.mat <- D.inv %*% Gamma0.mat %*% D.inv
rho_0.dif <- Rho0.hat - Rho0.mat
colnames(rho_0.dif) <- c('z_1', 'z_2')
rownames(rho_0.dif) <- c('z_1', 'z_2')
rho_0.dif
```

# Exercise 2: Checking VAR(1) Stationarity

Recall the conditions to check if a VAR(1) process is stationary. Now assume the VAR(1) model $z_{t} = \phi_{1} z_{t-1} + a_{t}$ with $a_{t}$ as a sequence of i.i.d. innovations:

\begin{itemize}
\item[a)] Do you need to make further assumptions on the cross-correlations of $a_{t}$ to ensure stationarity?
\end{itemize}

_Solution:_ 

\begin{align*}
  Z_t & = \phi_1 Z_{t-1} + a_t\\
  a_t & \overset{i.i.d.}{\sim} \left[ \mu_a , \Sigma_a \right]\\
  i.i.d.: & \Cov (a_t , a_{t-1}) = 0
\end{align*}

Generally not, since i.i.d. errors induce no dynamic structure. Still, finite $1^{st}$ and $2^{nd}$ moments are required for weak stationarity! (Gaussian innovations fulfill that condition, of course).


\begin{itemize}
 \item[b)] Which of the following processes are stationary? $\phi_{1} = \dots $ \\
\begin{itemize}	
	\item[i)] $ \begin{pmatrix}
	0.2 & 0.3 \\ 
	-0.6 & 1.1
	\end{pmatrix} $
	\hspace{8em}
	\item[ii)] $ \begin{pmatrix}
	0.5 & 0.3 \\ 
	0 & -0.3
	\end{pmatrix} $
	%\hspace{8em}
	\item[iii)] $ \begin{pmatrix}
	1 & 0 \\ 
	0 & 1
	\end{pmatrix} $
	\hspace{8em}
	\item[iv)] $ \begin{pmatrix}
	1 & -1 \\ 
	1 & -1
	\end{pmatrix} $
	\hspace{8em}
	\item[v)] $ \begin{pmatrix}
	1 & -0.5 \\ 
	-0.5 & 0
	\end{pmatrix} $
\end{itemize}
\end{itemize}

_Solution:_ 

\begin{align*}
  Z_t & = \phi_1 z_{t-1} + a_t\\
  & = \phi_1 \cdot(\phi_1 z_{t-2} + a_{t-1}) + a_t \\
    & = \underbrace{ \phi_1^p \ z_{t-p}}_{\text{stable ?}} + \underbrace{\sum_{i = 0}^{p} \phi_1^{i} \ a_{t -1} }_{\text{summable ?}}
\end{align*}
\begin{align*}
  & \lim_{p \rightarrow \infty} \phi_1^p \longrightarrow 0_{k \times k} \\
  & \Rightarrow \text{eigenvalues !} \\
  & \Rightarrow \phi_1 x = \lambda x \Rightarrow \phi_1^{p} x = \lambda^p x\\
  & \Rightarrow \text{solve:} \left( \phi_1 - I_K \lambda \right) x = 0\\
  & \text{for} \ x \neq 0: \left| \phi_1 - I_K \lambda \right| \overset{!}{=} 0 \\
  & \text{stability (for stationarity)}  \left| \lambda_1 \right|, \ldots,  \left| \lambda_k \right| < 1 \\
\end{align*}


\begin{itemize}	
	\item[i)] $\begin{pmatrix}
	    0.2 & 0.3 \\ 
	    -0.6 & 1.1 \\
	  \end{pmatrix} $
\end{itemize}

\begin{align*}
   = & \begin{pmatrix}
	    0.2 & 0.3 \\ 
	    -0.6 & 1.1 \\
	  \end{pmatrix} - 
	  \begin{pmatrix}
	   \lambda & 0 \\ 
	   0 & \lambda
	  \end{pmatrix}\\
	  = &  
	  \left|
	  \begin{matrix}
      (0.2 - \lambda) & 0.3 \\ 
	    -0.6 & (1.1 - \lambda)
	  \end{matrix}
	  \right| \\
	  = & (0.2 -\lambda)(1.1 - \lambda) - (-0.6) (0.3)\\
	  = & \lambda^2 - 1.3 \lambda + 0.4 \overset{!}{=} 0 \\
	  pq \text{-formula} \Rightarrow  \lambda_{1,2} = & - \left( \dfrac{-1.3}{2} \right) \pm \sqrt{ \left( \dfrac{-1.3}{2} \right)^2 - 0.4 }\\
	   = & 0.65 \pm 0.15 \\
	  \lambda_1 = & 0.8 \\
	  \lambda_2 = & 0.5 \\
	  & |\lambda_1| < 1 , |\lambda_2| < 1,  \; \text{stationary}
\end{align*}

\begin{itemize}	
	\item[ii)] $ \begin{pmatrix}
	0.5 & 0.3 \\ 
	0 & -0.3
	\end{pmatrix} $
\end{itemize}	
	
\begin{align*}
  & \left|
  \begin{matrix}
	0.5 - \lambda & 0.3 \\ 
	0 &  -0.3 - \lambda
	\end{matrix}
	\right| \\
	= & (0.5 - \lambda)(-0.3 - \lambda) - 0.3 \cdot 0 \overset{!}{=} 0 \\
	\Rightarrow & \lambda_1 = 0.5, \; \lambda_2 = 0.3 \Rightarrow \text{stationary} 
\end{align*}
	
	
\begin{itemize}		
		\item[iii)] $ \begin{pmatrix}
	1 & 0 \\ 
	0 & 1
	\end{pmatrix} $
\end{itemize}

\begin{align*}
  & \left|
  \begin{matrix}
	1 - \lambda & 0 \\ 
	0 &  1 - \lambda
	\end{matrix}
	\right| \\
	= & (1 - \lambda)(1 - \lambda) - 0 \cdot 0 \overset{!}{=} 0 \\
	\Rightarrow & \lambda_1 = 1, \; \lambda_2 = 1 \Rightarrow \text{not stationary} 
\end{align*}

\begin{itemize}
	\item[iv)] $ \begin{pmatrix}
	1 & -1 \\ 
	1 & -1
	\end{pmatrix} $
\end{itemize}

\begin{align*}
  & \left|
  \begin{matrix}
	1 - \lambda & -1 \\ 
	1 &  -1 - \lambda
	\end{matrix}
	\right| \\
	= & (1 - \lambda)(-1 - \lambda) - 0 \cdot 0 \overset{!}{=} 0 \\
	= & \lambda^2 + \lambda - \lambda - 1 +1 \\
	= & \lambda^2 \overset{!}{=} 0\\
	\Rightarrow & \lambda_1 = 0, \; \lambda_2 = 0 \Rightarrow \text{ stationary} 
\end{align*}

\begin{itemize}
	\item[v)] $ \begin{pmatrix}
	1 & -0.5 \\ 
	-0.5 & 0
	\end{pmatrix} $
\end{itemize}

\begin{align*}
  & \left|
  \begin{matrix}
	1 - \lambda & -0.5 \\ 
	-0.5 &  0 - \lambda
	\end{matrix}
	\right| \\
	= & (1 - \lambda)( - \lambda) - 0.5 \cdot 0.5 \overset{!}{=} 0 \\
	= & \lambda^2 - \lambda - 0.25 \overset{!}{=} 0\\
	\Rightarrow & \lambda_1 = 0.207, \; \lambda_2 = 1.207 \Rightarrow \text{not stationary} 
\end{align*}






