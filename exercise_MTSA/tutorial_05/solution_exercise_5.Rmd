---
title: 'Multivariate Time Series Analysis'
author: 'Dr. Yannick Hoga'
author2: 'Thilo Reinschlüssel'
subtitle: 'Solution Exercise Sheet 5'
semester: "Winter Term 2019/2020"
output:
  pdf_document:
    keep_tex: yes
    template: ../template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
linkcolor: black
urlcolor: black
citecolor: black
colorlinks: true
font: Times New Roman
fontsize: 12pt
geometry: lmargin = 2cm, rmargin = 2cm, tmargin = 2cm, bmargin = 2.5cm
classoption: a4paper
---

```{r , include=FALSE}
Sys.setlocale(locale = "English_United States.1252") ## English US Windows
knitr::opts_chunk$set(echo = TRUE)

source(here::here("packages/packages.R"))
```

# Exercise 1: Information Criteria

Prove Corollary 4.5 from Slide 4-7.


_Solution:_

From Theorem 4.4:

\begin{align*}
  C(l) = \log \left(\hat{\Sigma_a} (l) \right) + \dfrac{l}{T} \cdot c_T
\end{align*}


\begin{itemize}
  \item[i)] $\lim \limits_{T \rightarrow \infty} c_T \longrightarrow \infty$ 
  \item[ii)] $\lim \limits_{T \rightarrow \infty} \dfrac{c_T}{T} \longrightarrow 0$
\end{itemize}

If i) and ii) hold, $C(l)$ chooses the optimal/correct model. 

\begin{itemize}
  \item AIC: $c_T  = 2K^2$
  \begin{align*}
    \lim \limits_{T \rightarrow \infty} c_T & = 2 \ K^2 \centernot\implies \infty
  \end{align*}
  \begin{itemize}
    \item[$\Rightarrow$] not consistent
  \end{itemize}
  \item BIC: $c_T = \log(T)  \cdot K^2$
    \begin{align*}
    \lim \limits_{T \rightarrow \infty} c_T & = \log(T) K^2   \implies \infty \\
    \lim \limits_{T \rightarrow \infty} \dfrac{c_T}{T} & = \dfrac{\log(T)}{T} K^2 \implies 0 
  \end{align*}
  \begin{itemize}
    \item[$\Rightarrow$] consistent
  \end{itemize}
  \item HQ: $c_T = 2 \ \log(\log(T)) \ K^2$
  \begin{align*}
    \lim \limits_{T \rightarrow \infty} c_T & = 2 \ \log(\log(T)) \ K^2 \implies \infty  \\
    \lim \limits_{T \rightarrow \infty} \dfrac{c_T}{T} & = \dfrac{2 \ \log(\log(T)) \ K^2}{T} \implies 0 
  \end{align*}
  \begin{itemize}
    \item[$\Rightarrow$] consistent
  \end{itemize}
\end{itemize}


# Exercise 2: VAR(p): Data application
  
This exercise is concerned with finding an appropriate $\VAR(p)$ model for US macroeconomic data. You can find the dataset `us_macrodata.Rda` attached to this exercise sheet in the Moodle folder for this tutorial. Please use the `load` command to import the dataset from your directory into R.  There are 5 variables – CPI, Real GDP, the unemployment rate, general private investment and the debt-to-GDP ratio. All series have been sampled quarterly and were seasonally adjusted before downloaded from FRED.

```{r}
# loading data
load(file = here::here("exercise_MTSA/00_data/us_macrodata.Rda"))
# loading the MTS package
library(MTS)
```

\begin{itemize}
  \item[a.)] Plot all time series and judge which time series seem non-stationary. Proceed to compute growth rates of the non-stationary variables.
\end{itemize}

_Solution:_

```{r}
macmat <- data.matrix(us.macro_series)
plot.ts(macmat)
```

Every series except unemployment looks non-stationary. Regarding the debt-to-gdp ratio, this is surprising, but we better difference it as well.


```{r}
macdata <- cbind(diff(log(us.macro_series$cpi)), 
                 us.macro_series$unemprate[-(nrow(macmat))], 
                 diff(log(us.macro_series$rgdp)),
                 diff(log(us.macro_series$gp_investment)), 
                 diff(log(us.macro_series$debt_gdp)))

plot.ts(macdata)
```

Note that the last observation of 'unemp' was dropped for conformable length. Its last and not first due to the date information: measurements are always from the first day of a quarter.


\begin{itemize}
  \item[b.)] Perform a Ljung-Box test on the dataset. Does it look worthwhile to estimate a $\VAR(p)$
\end{itemize}

_Solution:_

```{r}
mq(x = macdata, lag = 20)
```

There is some correlation in the dataset. 


```{r}
mq(x = macdata[,-3], lag = 20)
```

Even without unemployment, there is some correlation in the dataset. 


\begin{itemize}
  \item[c.)] Determine the length of the time series. How many coefficients can be estimated and what does it mean for $K$ and $p$?
\end{itemize}

_Solution:_

We have $T \cdot K$ data points and we estimate $K^2$ parameters for each lag. For the intercept we estimate $K$ parameters. Which leads to the following condition for the maximal number of lag(s) $p$: 

$$ \dfrac{K \cdot (T -1)}{K^2} \geq p$$

```{r}
data_dim <- dim(macdata) 

Tmax <- data_dim[1] # observations
K <- data_dim[2] # variables
(max.p <- (Tmax * K - K) / K^2 )
```

`r floor(max.p)` lags can be estimated in addition to the intercept. 


\begin{itemize}
  \item[d.)] Consult the AIC, BIC and HQ to determine the optimal lag order for a $\VAR(p)$ model for the whole dataset. Plot the values of the three criteria for the lag orders p from 1 to 5 in one plot.
\end{itemize}

_Solution:_

```{r}
M <- 10 # maximal p 
VARorder(x = macdata, maxp = M)
# so it's p = 1, 2 or 4.....
var1to5.fit <- lapply(X = 1:M, function(i)
  VAR(x = macdata, include.mean = TRUE, p = i, output = FALSE))
var1to5.aic <- sapply(1:M, function(i) var1to5.fit[[i]]$aic)
var1to5.bic <- sapply(1:M, function(i) var1to5.fit[[i]]$bic)
var1to5.hq <- sapply(1:M, function(i) var1to5.fit[[i]]$hq)
plot(x = 1:M, y = var1to5.aic, type = "b",
     main = "Values of Information Criteria", xlab = "p",
     ylim = c( min(c(var1to5.aic, var1to5.bic, var1to5.hq)), 
               max(c(var1to5.aic, var1to5.bic, var1to5.hq)) ) )

points(x = 1:M, y = var1to5.bic, pch = 11, col = "red")
points(x = 1:M, y = var1to5.hq, col = "blue", pch = 25)
legend("topleft", legend = c("AIC", "BIC", "HQ"), 
       col = c("black", "red", "blue"), 
       pch = c(NA, 11, 25), lwd = c(2, NA, NA))
```

\begin{itemize}
  \item AIC and HQ are flat around the minima $\Rightarrow$ no distinct optimum visible. 
  \item Conceivable reasons: persistence, omitted variables,  wrong functional form
  \item The $\VAR$ may just work as an approximation
  \item BIC is the most conservative IC
  \item Mimima at:
\end{itemize}

\begin{align*}
p = 
  \begin{cases}
  1 & BIC \\
  2 & HQ \\
  4 & AIC
  \end{cases}
\end{align*}

\begin{itemize}
  \item[e.)] Fit $\VAR(p)$ models incorporating all variables using the optimal lag order(s) p suggested by each of the information criteria. Apply the Ljung-Box test to inspect the residuals’ properties. For which models does the test reject the null hypothesis on one of the first ten lags?
\end{itemize}


_Solution:_

First we need to estimated a $\VAR(p)$. 
```{r, results = 'hold'}
var1.fit <- VAR(x = macdata, p = 1, include.mean = TRUE, output = FALSE)
var2.fit <- VAR(x = macdata, p = 2, include.mean = TRUE, output = FALSE)
var4.fit <- VAR(x = macdata, p = 4, include.mean = TRUE, output = FALSE)
```

Now the Ljung-Box test can be performed. We adjust using $K = 5$ with $5^2 \times p$ degree of freedom. Adjustment for the intercept is not necessary, since $z_t$ needs to be demeaned anyway $\left(\Gamma_0 = \left(z_t - \mu \right) \left( z_t - \mu \right)^{'} \right)$.  The Ljun-Box test has $m \times K^2$ degree of freedom ($K^2$ per lagged cross-correlation matrices), so after adjustment, we have $(m - p) K^2$ degrees of freedom. 

```{r, results = 'hold'}
mq(x = var1.fit$residuals, lag = 25, adj = 25 * 1)
mq(x = var2.fit$residuals, lag = 25, adj = 25 * 2)
mq(x = var4.fit$residuals, lag = 25, adj = 25 * 4)
```

Since the tests rejects everywhere else, the VARs do not explain the dynamics entirely. 


\begin{itemize}
  \item[f.)] Now take a $\VAR(1)$ and a $\VAR(4)$ model with all variables included and an intercept specified.
  \begin{enumerate}[label=(\roman*)]
    \item How many coefficients are estimated in each case?
    \item Look at both estimates of $\Sigma_a$ - are there major difference?
    \item Compare the standard errors associated with the $\phi_1$ matrices of the $\VAR(1)$ and $\VAR(4)$ from above. Do you see the same pattern regarding $\Sigma_a$? 
  \end{enumerate}
\end{itemize}

_Solution:_

  \begin{enumerate}[label=(\roman*)]
    \item How many coefficients are estimated in each case?
    
    The formula for computing the number of parameters is $K^2 \times p + K$. For a $\VAR(1)$ we estimate `r 5 + 5^2 * 1` parameters. Whereas for a $\VAR(4)$ we already need to estimate  `r 5 + 5^2 * 4` parameters. 
    
    \item Look at both estimates of $\Sigma_a$ - are there major difference?

    Ratio of residual coveriances: 
  \end{enumerate}
  
```{r, results = 'hold'}
var1.fit$Sigma / var4.fit$Sigma
```

Reisdual (co)variances are higher for the $\VAR(1)$. $\VAR(4)$ predicts better (in-sample).

  \begin{enumerate}[label=(\roman*)]
    \setcounter{enumi}{2}
    \item Compare the standard errors associated with the $\phi_1$ matrices of the $\VAR(1)$ and $\VAR(4)$ from above. Do you see the same pattern regarding $\Sigma_a$? 
  \end{enumerate}

Ratio of standard errors:

```{r, results = 'hold'}
var1.fit$secoef / var4.fit$secoef[1:6,]
```

No, it is exactly the other way around. Estimating more coefficients with the same information leads to less information per coefficient. 

\begin{itemize}
  \item[g.)]  Repeat the task from above with CPI and the debt-to-gdp ratio as the only variables (hence $K = 2$). How many coefficients are estimated in this case?
\end{itemize}

With only two explanatory variables we can estimated maximally `r (Tmax * 2 - 2) / 2^2` lags.  

```{r, results = 'hold'}
var1red.fit <- VAR(x = macdata[,c(1,5)], p = 1, output = FALSE, include.mean = TRUE)
var4red.fit <- VAR(x = macdata[,c(1,5)], p = 4, output = FALSE, include.mean = TRUE)
```

For a $\VAR(1)$  `r 2 + 2^2 * 1` coefficients are estimated, for a $\VAR(4)$ `r 2 + 2^2 * 4` coefficients. 

Any major differences between the residual covariance matrices?

Ratio of residual coveriances:
```{r, results = 'hold'}
var1red.fit$Sigma / var4red.fit$Sigma
```

Ratio of standard errors:
```{r, results = 'hold'}
var1red.fit$secoef / var4red.fit$secoef[1:3,]
```

\begin{itemize}
  \item[h.)] At last, go back to $\VAR(1)$ and $\VAR(4)$ models from task f). Use the standard error matrices to compute t-statistics for each coefficient with the null hypothesis $H_0: phi_(p,jk) = 0$. How often is the null hypothesis rejected at the 5\% level in each of the two models? 
\end{itemize}

To compute the t-statistic the estimated parameters get divided by the standard error of the parameter.  

```{r, results = 'hold'}
var1.t_ratios <- var1.fit$coef / var1.fit$secoef
var2.t_ratios <- var2.fit$coef / var2.fit$secoef
var4.t_ratios <- var4.fit$coef / var4.fit$secoef
```

To test how often the $H_0$ is rejected we just count how often the t-value is absolute greater than 1.96 (`sum(abs(var1.t_ratios ) > 1.96)`). For the $\VAR(1)$ the $H_0$ is `r sum(abs(var1.t_ratios ) > 1.96)` times which are  `r round( (sum(abs(var1.t_ratios ) > 1.96)) / (5 + 5^2 * 1), 4)` of all parameters. For a $\VAR(2)$ `r sum(abs(var2.t_ratios ) > 1.96)` times the null hypothesis is rejected (`r round( (sum(abs(var2.t_ratios ) > 1.96)) / (5 + 5^2 * 2), 4)`) and for a $\VAR(4)$ `r sum(abs(var4.t_ratios ) > 1.96)` times (`r round( (sum(abs(var4.t_ratios ) > 1.96)) / (5 + 5^2 * 4), 4)`). 

Same pattern as in f), but not that pronounced this time. 

# Exercise 3: This exercise is concerned with predicting growth rates of exchange rates. Please download the file `quandl_fx_download.R` from the Moodle and install the package `Quandl`. Executing the script will then download and prepare the two time series we are interested in.  


To download the data from `Quandl` you need your own API key and execture the following code.  

```{r, eval = FALSE}
library(Quandl)
# Set API key
Quandl.api_key("") # Please enter your key in here.

## Download and prepare data

# Download daily data on Japan/US FX rates
FX.Ja   <- Quandl("FRED/DEXJPUS", start_date = "1998-12-30", 
                  end_date = "2018-12-31", type = "xts")   
# Download daily data on Euro/US FX rates
FX.Eu   <- Quandl("FRED/DEXUSEU", start_date = "1998-12-30", 
                  end_date = "2018-12-31", type = "xts")  

# Compute Growth rates
lr.Eu <- diff(log(FX.Eu[, 1]))[-1] # leave out NA in first component via [-1]
lr.Ja <- diff(log(FX.Ja[, 1]))[-1] # leave out NA in first component via [-1]

# only use data when both log-returns are available
V         <- merge.xts(lr.Eu, lr.Ja, all=FALSE)  

date      <- index(V)   # save dates for later use
fx_series <- coredata(V)  # raw log-returns
N         <- length(date)
```

```{r, include = FALSE}
load(here::here('exercise_MTSA/00_data/quandl.Rda'))
```

\begin{itemize}
  \item[a.)] Apply the Ljung-Box test on the multivariate time series and comment. 
\end{itemize}

```{r, results = 'hold'}
mq(x = fx_series, lag = 35)
```

Except those correlations around lag $20$ to $24$, there seems to be no commanding dynamic pattern here.

\begin{itemize}
  \item[b.)] Do the usual information criteria support the finding of the Ljung-Box test? 
\end{itemize}

```{r, results = 'hold'}
VARorder(x = fx_series, maxp = 35)
```

The information criterias (AIC, BIC, and HQ) support the findings of the Ljung-Box test. 

\begin{itemize}
  \item[c.)] Regardless of a) and b), fit a $\VAR(1)$ to the time series. Compare $\Sigma_a$ with $\Gamma_0$. 
\end{itemize}

```{r}
fx_var1.fit <- VAR(x = fx_series, p = 1, include.mean = TRUE, output = FALSE)
( Gamma_0 <- cov(fx_series))
(fx_var1.fit$Sigma)
fx_var1.fit$Sigma / Gamma_0
```

No important difference. here is almost no variation taken away by the $\VAR(1)$. 


\begin{itemize}
  \item[d.)] Apply the Ljung-Box test on the residuals. Do the results surprise you? 
\end{itemize}

```{r}
mq(x = fx_var1.fit$residuals, lag = 35, adj = 2^2 * 1)
```

No. c) has shown that nothing has changed at all. 

\begin{itemize}
  \item[e.)] Repeat the Ljung-Box test but this time with the squared residuals. Also have a look at the information criteria. 
\end{itemize}

```{r}
mq(x = fx_var1.fit$residuals^2, lag = 50, adj = 2^2 * 1)
VARorder(x = fx_var1.fit$residuals^2, maxp = 20)
plot.ts(fx_var1.fit$residuals^2)
```

Plenty of lagged cross-/auto correlations. Information criteria suggest high lag orders. Fitting a $\VAR$ appears sensible. 

\begin{itemize}
  \item[f.)] Can you rule out weak stationarity for the growth rates of the exchange rates only based on your findings up to this point?
\end{itemize}

No. Weak stationarity is about time-invariance regarding the *unconditional* expecation and variance. Heteroscedasticity can be a violation of weak stationarity, simply because the variance is not constant over time. But if we are able to sufficiently model the variation of the variance using a VAR, we are in fact facing *conditional* heteroscedasticity: $E(a_{t}^2) = \sigma_{t} * \epsilon_{t}$ with $\epsilon_{t}$ as white noise.  And as we learned before, a stable VAR with white noise-innovations yields a stationary time series.
This is analogous to a stationary VAR, where the conditional expectation of the observations may differ from the unconditional expectation (depending on past observations).


