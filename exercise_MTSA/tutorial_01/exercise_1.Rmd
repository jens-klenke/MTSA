---
title: 'Multivariate Time Series Analysis'
author: 'Dr. Yannick Hoga'
author2: 'Thilo Reinschlüssel'
subtitle: 'Exercise Sheet 1'
semester: "Winter Term 2019/2020"
output:
  pdf_document:
    keep_tex: yes
    template: ../template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
linkcolor: black
urlcolor: black
citecolor: black
colorlinks: true
font: Times New Roman
fontsize: 12pt
geometry: lmargin = 2cm, rmargin = 2.5cm, tmargin = 2cm, bmargin = 2.5cm
classoption: a4paper
---

```{r , include=FALSE}
Sys.setlocale(locale = "English_United States.1252") ## English US Windows
knitr::opts_chunk$set(echo = TRUE)

#options(kableExtra.latex.load_packages = FALSE)
#### required packages ####
source(here::here("packages/packages.R"))
```

# Exercise 1: Matrix Operations


Prove properties 3,4 and 5 from Proposition 1.2 (Slide 1-11). Are there any requirements regarding
the matrix dimensions?


_Solution:_ \

\begin{itemize}
    \item[i)] Property 3: $(A \otimes B)(F \otimes G) = (AF) \otimes (BG)$
    \begin{align*}
      \text{Let} \ A & = 
      \begin{pmatrix}
      a_{11} & \ldots & a_{1q}\\
      \vdots & \ddots & \vdots \\
      a_{p1} & \ldots & a_{pq}\\
      \end{pmatrix}
      \text{and} \ F = 
      \begin{pmatrix}
      f_{11} & \ldots & f_{1n}\\
      \vdots & \ddots & \vdots \\
      f_{m1} & \ldots & f_{mn}\\
      \end{pmatrix}\\
      \text{hence} (A \otimes B) & = 
      \begin{pmatrix}
      a_{11}B & \ldots & a_{1q}B\\
      \vdots & \ddots & \vdots \\
      a_{p1}B & \ldots & a_{pq}B\\
      \end{pmatrix}
      \text{and} \ (F \otimes G) \ \text{analsgously}
    \end{align*}
    \begin{align*}
    (A \otimes B)(F \otimes G) & = 
    \begin{pmatrix}
      a_{11}B & \ldots & a_{1q}B\\
      \vdots & \ddots & \vdots \\
      a_{p1}B & \ldots & a_{pq}B\\
    \end{pmatrix}
    \begin{pmatrix}
      f_{11}G & \ldots & f_{1n}G\\
      \vdots & \ddots & \vdots \\
      f_{m1}G & \ldots & f_{mn}G\\
    \end{pmatrix}\\
    & = 
    \begin{pmatrix}
      (a_{11}B f_{11}G +\ldots + a_{1q}B f_{m1}G) & \ldots & (a_{11}B f_{1n}G +\ldots + a_{1q}B f_{mn}G)\\
      \vdots & \ddots & \vdots \\
      (a_{p1}B f_{11}G +\ldots + a_{pq}B f_{m1}G) & \ldots & (a_{p1}B f_{1n}G +\ldots + a_{pq}B f_{mn}G)\\
    \end{pmatrix}\\
     & = 
    \begin{pmatrix}
      (a_{11} f_{11} +\ldots + a_{1q} f_{m1}) & \ldots & (a_{11} f_{1n} +\ldots + a_{1q} f_{mn})\\
      \vdots & \ddots & \vdots \\
      (a_{p1} f_{11} +\ldots + a_{pq} f_{m1}) & \ldots & (a_{p1} f_{1n} +\ldots + a_{pq} f_{mn})\\
    \end{pmatrix} \otimes (BG) \\
    \end{align*}
    \begin{align*}
    & = 
    \begin{pmatrix}
      \sum_{i = 1}^{q = m} a_{1i} f_{i1} & \ldots &  \sum_{i = 1}^{q = m} a_{1i} f_{1i} \\
      \vdots & \ddots & \vdots \\
      \sum_{i = 1}^{q = m} a_{pi} f_{i1} & \ldots & \sum_{i = 1}^{q = m} a_{pi} f_{in}\\
    \end{pmatrix} \otimes (BG) \qquad \quad \\
    & = (AF) \otimes (BG) 
    \end{align*}
    Dimensions: 
      \begin{table}[h]
      \centering
      \begin{tabular}{|l|l|}
        \hline
          $A: p \times q$ & $F: m \times n$ \\ \hline
          $B: c \times d$ & $G: h \times k$ \\ \hline
      \end{tabular}
      \end{table}
    \begin{itemize}
      \item[$\Rightarrow$] $dim(A \otimes B) = pc \times qd, dim(F \otimes G) = mh \times kn$
    \end{itemize}
    \item[ii)] Property 4: $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$ \\
    $\Rightarrow$ Claim and verify \\
    The inverse is defined as following: \\
    $(A \otimes B)(A \otimes B)^{-1} = I$ where $I$ is the identitiy matrix\\
    Then $(A \otimes B)(A^{-1} \otimes B^{-1}) = I$ must hold if the claim was true\\
    We know from Property 3 that $(A \otimes B)(A^{-1} \otimes B^{-1}) = (AA^{-1} \otimes BB^{-1}) =  I \otimes I = I$ \\
    Dimensions: $A$ and $B$ must be non-singular square matrices
    \item[iii)] Property 3: $tr(A \otimes C ) = tr(A) \cdot tr( C)$ for square matrices $A$ and $C$
    \begin{align*}
      tr(A \otimes C) = 
      tr 
      \begin{pmatrix}
        a_{11}C & \ldots & a_{1n}C \\
        \vdots & \ddots & \vdots \\
        a_{n1}C & \ldots & a_{nn}C
      \end{pmatrix}
      = \sum_{i = 1}^{n}\left( a_{ii} tr(C) \right) = tr(C) \sum_{i=1}^{n} a_{ii} = tr(C) tr(A)
    \end{align*}
\end{itemize}

# Exercise 2: Bivariate Functions

Find the extrema of the following functions (using pen and paper). Determine whether these points constitute minima, maxima or saddle points:

\begin{itemize}
    \item[a)] $f(x,y) = (x -2)^2 + (y -5)^2 + xy$
    \item[b)] $g(x,y) = (x -1)^2 - (4y + 1)^2$
\end{itemize}

_Solution:_ 

Solution concept:
\begin{enumerate}
    \item FOC: first derivatives $\overset{!}{=} 0$
    \item SOC: Inspectthe Hessian matrix
    \begin{itemize}
      \item[$\Rightarrow$] $f_{xx}, f_{yy} < (>) \ 0$ for maxima (minima), but also $f_{xy}, f_{yx}$ matter. Ultimatley, eigenvalues are need to judge correctly in complex situations. 
    \end{itemize}
\end{enumerate}


\begin{itemize}
    \item[a)] $f(x,y) = (x -2)^2 + (y -5)^2 + xy$
    \begin{align*}
      f(x,y) & = (x -2)^2 + (y - 5)^2 + xy\\
      & \\
      \dfrac{\partial f(x,y)}{\partial x} & = 2(x -2) + y \overset{!}{=} 0 \\
      \dfrac{\partial f(x,y)}{\partial y} & = 2(y -5) + x \overset{!}{=} 0 
    \end{align*}
    \begin{itemize}
      \item Solving the equation system yields:
    \end{itemize}
    \begin{align*}
      x = 2 -\dfrac{y}{2} \Rightarrow 2y - 10 + 2 - \dfrac{y}{2} = 0 &\Rightarrow y^{*} = \dfrac{16}{3} \\
      &\Rightarrow x^{*} = 2 -\dfrac{16}{3 \cdot 2} = - \dfrac{2}{3}
    \end{align*}
    \begin{itemize}
      \item Evaluating the Hessian matrix:
    \end{itemize}
    \begin{align*}
      \dfrac{\partial f(x,y)}{\partial x^2} = 2 \qquad  \dfrac{\partial f(x,y)}{\partial xy} = 1 \\
      \dfrac{\partial f(x,y)}{\partial yx} = 1  \qquad  \dfrac{\partial f(x,y)}{\partial y^2} = 2\\
      \Rightarrow H = 
      \begin{pmatrix}
        2 & 1 \\
        1 & 2
      \end{pmatrix}
    \end{align*}
    \begin{itemize}
      \item[1.] $f_{xx}, f_{yy} > 0 \Rightarrow$ minimum is possible
      \item[2.] $\left| H(x,y) \right|_{x = x^{*}, y = y^{*}} = 2 \cdot 2 - 1 \cdot 1 = 3 \; \Rightarrow$ That constitutes a minimum
      \item[3.] $\lambda_1 = 3, \lambda_2 = 1 \; \Rightarrow$ both eigenvalues are positive which is nor surpising since the determinant is also positive
      \item[$\Rightarrow$] \underline{\textit{Note}} that 1. and 2. are sufficient arguments in this case where $\left| H(x,y) \right|$ is \underline{positive definite}
    \end{itemize}
    \item[b)] $g(x,y) = (x -1)^2 - (4y + 1)^2$
    \begin{align*}
      g(x,y) & = (x -1)^3 + (4y - 1)^2\\
      & \\
      \dfrac{\partial g(x,y)}{\partial x} & = 2(x - 1) \overset{!}{=} 0 \Leftrightarrow x^{*} = 1 \\
      \dfrac{\partial g(x,y)}{\partial y} & = - 2 \cdot 4 (4y + 1) + x \overset{!}{=} 0 \Leftrightarrow y^{*} = - \dfrac{1}{4} 
    \end{align*}
    \begin{itemize}
      \item Evaluating the Hessian matrix:
    \end{itemize}
    \begin{align*}
    	\begin{matrix}
      \dfrac{\partial g(x,y)}{\partial x^2} = 2 \qquad  & \dfrac{\partial g(x,y)}{\partial xy} = 0 \\
      \dfrac{\partial g(x,y)}{\partial yx} = 0  \qquad  & \dfrac{\partial g(x,y)}{\partial y^2} = -32
      \end{matrix}\\
      \Rightarrow \left| H \right|_{x = 1, y = - 0,25} = 
      \begin{pmatrix}
        2 & 0 \\
        0 & -32
      \end{pmatrix}
    \end{align*}
    \begin{itemize}
      \item[1.] $f_{xx}, f_{yy} < 0 \; \Rightarrow$ no clear minimum / maximum
      \item[2.] $|H| = 2 \cdot (-32) - 0 \cdot 0 = - 64 < 0 \; \Rightarrow$ indicates a saddle point in the case of two variables!
      \item[3.] $\lambda_1 = 2, \lambda_2 = -32 \; \Rightarrow$ One postive eigenvalue, one negative eigenvalue is sufficient for a saddle point 
    \end{itemize}
\end{itemize}

# Exercise 3:  Stationarity

\begin{itemize}
    \item[a)] Are weakly stationary processes always strictly stationary? Construct an example to support your argument
    \item[b)] Is weak stationarity a necessary condition for strict stationarity? Bring an example.
    \item[] \textit{Hint: How many moments does a distribution require?}
\end{itemize}

_Solution:_ 

\begin{itemize}
    \item[a)] No. A time series of length $T$ drawing from $N(0, 1)$ for $t \in \left[0, \dfrac{T}{2} \right]$ and drawing from Student’s t-distribution for $t \in \left(\dfrac{T}{2}, T \right]$ has a constant mean $\mu = 0$ and variance $\sigma^2 = 1$, but the kurtosis ($4^{th}$ moment) changes throughout time. In consequence the joint distribution of a subsequence $x_{t-p}, \ldots , x_{t+p}$ is not independent of $t$. Therefore it is not strictly stationary
    \item[b)] No. Take the Cauchy distribution as an example: $f(x) = \dfrac{1}{\pi} \cdot \dfrac{s}{s^2 + (x -t)^2}$. Any $i.i.d.$ sample from this distribution would be obviously strictly stationary. Yet this distribution has no existing moments at all (the integral diverges), hence it cannot exhibit a constant expected value or variance over time. Therefore it is only strictly stationary, but not weakly stationary! (Other example: $t_1$ distribution, where only the mean but not the variance exists).
\end{itemize}

# Exercise 4:  Covariance Matrices under Stationarity

Referring to Remark 1.13: Show that $\Gamma_l = \Gamma^{T}_{-l}$ holds for all weakly stationary processes.

(_Two dimensions suffice_)

_Solution:_ 

Without loss of generality assume $\mu = 0$ everywhere and assume $z$ to be a bivariate
vector $(x,y)^T$. Let $\Gamma_{l,t}$ be the covariance matrix of the $l^{th}$ lag at time $t$:

\begin{align*}
\Gamma_{l,t} = 
\begin{bmatrix}
  \mathbb{E}(x_t \cdot x_{t-l}) & \mathbb{E}(x_t \cdot y_{t-l}) \\
  \mathbb{E}(y_t \cdot x_{t-l}) & \mathbb{E}(y_t \cdot y_{t-l}) \\
\end{bmatrix}
\quad \text{and} \quad 
\Gamma_{l,t}^{T} = 
\begin{bmatrix}
  \mathbb{E}(x_{t-l} \cdot x_t ) & \mathbb{E}(x_{t-l} \cdot y_t) \\
  \mathbb{E}(y_{t-l} \cdot x_t  ) & \mathbb{E}( y_{t-l} \cdot y_t) \\
\end{bmatrix}
= \Gamma_{-l, t- l}
\end{align*}

Since weak stationarity has been assumed, the covariance matrix is constant across time and $\Gamma_{-l, t- l} = \Gamma_{-l} = \Gamma_{l}^{T}$ and vice versa.

# Exercise 5: Ljung-Box Test in R

Load the package _MTS_ and open the associated data pool ’mts-examples’ (Slide 1-8). We are interested in the time series ’GS’, ’MS’ and ’JPM’ from the dataset ’tenstocks’:

\begin{itemize}
  \item[a)] First apply the Ljung-Box test on each time series individually. What do the results imply?
  \item[b)] Now apply the multivariate Ljung-Box test on all three time series together. Compare the results with those from the univariate test and comment on it.
\end {itemize}

_Solution:_ 

Firstly, we need to importe the example datasets from the _MTS_ package, which includes the tenstocks data set. 
```{r, eval = TRUE}
data("mts-examples")
```

If we take a look at the description of the _tenstock_ dataset^[To access help-file: `?tenstock()`], we can see that it contains `r ncol(tenstocks)` variables and `r nrow(tenstocks)` observations on  monthly simple returns from January 2001 to December 2011 for each of the `r ncol(tenstocks) -1` companies (first variable is the time vector).   

\begin{itemize}
  \item[a)] $\quad$
\end {itemize}

First, we will performe the Ljung-Box-test for the simple returns of _JP-Morgan Chase & Co. (JPM)_ company. 

```{r, message = FALSE, warning = FALSE, fig.height = 4}
mq(x = tenstocks$JPM, lag = 20) 
```

\FloatBarrier
The simple returns for _JP-Morgan Chase & Co. (JPM)_ show no autocorrelation for the first 20 lags to a significance level of 5 percent.   

Next, we will have a look at the test for the time series for _Morgan Stanley (MS)_.
\FloatBarrier

```{r, message = FALSE, warning = FALSE, fig.height = 4}
mq(x = tenstocks$MS, lag = 20) 
```

The results for the _MS_ time series are similar to those of _JPM_.


Lastly, there is only the time series from _Goldman Sachs Group Inc (GS)_ left to be analysed. 

```{r, message = FALSE, warning = FALSE, fig.height = 4}
mq(x = tenstocks$GS, lag = 20)
```

Only the first lag is _relatively_ close to be significant at a 5 percent significance level.

\begin{itemize}
  \item[b)] $\quad$
\end {itemize}

Now we take a look at the combined Ljung-Box test. 

```{r, message = FALSE, warning = FALSE, fig.height = 4}
mq(x = cbind(tenstocks$JPM, tenstocks$MS, tenstocks$GS), lag = 20)
```

All p-values are below 5%. Since the time series are not much autocorrelated (univariate!), there must be cross-correlations which cause the Null hypothesis to be rejected. So there is a dynamic pattern which might be explained using multivariate time series models.

Lets have a look at the correlation as a second look. 

```{r, eval = FALSE}
ccf(x = tenstocks$JPM, y = tenstocks$MS, lag.max = 20)
ccf(y = tenstocks$JPM, x = tenstocks$MS, lag.max = 20) 
ccf(x = tenstocks$JPM, y = tenstocks$GS, lag.max = 20)
ccf(x = tenstocks$MS, y = tenstocks$GS, lag.max = 20)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 4.5}
par(mfrow=c(2,2))
ccf(x = tenstocks$JPM, y = tenstocks$MS, lag.max = 20)
ccf(y = tenstocks$JPM, x = tenstocks$MS, lag.max = 20) 
ccf(x = tenstocks$JPM, y = tenstocks$GS, lag.max = 20)
ccf(x = tenstocks$MS, y = tenstocks$GS, lag.max = 20)
par(mfrow=c(1,1))
```

But keep in mind that the _Ljung-Box_ test does not take $\rho_0$ into consideration.

Lastly, we will plot the times series with the command `plot.ts()`.  

```{r, message = FALSE, warning = FALSE, fig.height = 4}
plot.ts(cbind(tenstocks$JPM, tenstocks$MS, tenstocks$GS)) 
```

